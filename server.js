require('dotenv').config();

const express = require('express');
const multer = require('multer');
const OpenAI = require('openai');
const cors = require('cors');

const app = express();
const PORT = process.env.PORT || 3000;

// Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ OpenAI ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
});

if (!process.env.OPENAI_API_KEY) {
    console.error('âš ï¸  WARNING: OPENAI_API_KEY Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½! Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.');
}

// Middleware - Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° CORS
app.use(cors({
    origin: '*',
    methods: ['GET', 'POST', 'OPTIONS'],
    allowedHeaders: ['Content-Type', 'Authorization', 'X-Session-Id', 'Accept'],
    exposedHeaders: ['Content-Type', 'X-Session-Id', 'Transfer-Encoding'],
    credentials: false,
    maxAge: 86400
}));

app.options('*', cors());
app.use(express.json());

// ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° multer Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
const upload = multer({ 
    storage: multer.memoryStorage(),
    limits: { fileSize: 25 * 1024 * 1024 } // 25MB (Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ OpenAI Whisper)
});

// Ğ¥Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² (Ğ¿Ğ¾ sessionId)
const conversations = new Map();

// Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°
function getConversationHistory(sessionId) {
    if (!conversations.has(sessionId)) {
        conversations.set(sessionId, []);
    }
    return conversations.get(sessionId);
}

// Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
function addToHistory(sessionId, role, content) {
    const history = getConversationHistory(sessionId);
    history.push({ role, content });
    // ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ 10 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸
    if (history.length > 10) {
        history.shift();
    }
}

// Streaming Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
async function processVoiceRequestStreaming(audioBuffer, sessionId, res) {
    try {
        console.log(`[${new Date().toISOString()}] Processing voice request (streaming) for session: ${sessionId}`);
        
        // Ğ¨Ğ°Ğ³ 1: Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Whisper
        console.log(`[${new Date().toISOString()}] Step 1: Transcribing audio with Whisper...`);
        
        const audioFile = new File([audioBuffer], 'audio.webm', { type: 'audio/webm' });
        
        const transcription = await openai.audio.transcriptions.create({
            file: audioFile,
            model: 'whisper-1',
            language: 'ru',
            response_format: 'text'
        });
        
        const userText = transcription.toString().trim();
        console.log(`[${new Date().toISOString()}] Transcription: "${userText}"`);
        
        if (!userText) {
            throw new Error('ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ');
        }

        // Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        addToHistory(sessionId, 'user', userText);

        // Ğ¨Ğ°Ğ³ 2: ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ GPT Ñ streaming
        console.log(`[${new Date().toISOString()}] Step 2: Getting streaming response from GPT...`);
        const history = getConversationHistory(sessionId);
        
        const messages = [
            {
                role: 'system',
                content: 'Ğ¢Ñ‹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾ Ğ´ĞµĞ»Ñƒ. ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'
            },
            ...history
        ];

        // Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ streaming Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ GPT
        const gptStartTime = Date.now();
        console.log(`[${new Date().toISOString()}] ğŸš€ Starting GPT streaming...`);
        
        const stream = await openai.chat.completions.create({
            model: 'gpt-4o-mini', // Ğ‘Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            messages: messages,
            stream: true,
            temperature: 0.7,
            max_tokens: 150 // Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
        });

        // Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼ Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ° TTS
        let accumulatedText = '';
        let fullText = '';
        let chunkCount = 0;
        let firstChunkTime = null;
        const MIN_CHARS_FOR_TTS = 8; // Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° TTS
        const MAX_WAIT_CHARS = 30; // Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
        
        // ĞÑ‡ĞµÑ€ĞµĞ´ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ TTS Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
        let ttsQueue = Promise.resolve();
        let ttsChunkCount = 0;
        let firstTtsSentTime = null;
        let isFirstTtsChunk = true; // Ğ¤Ğ»Ğ°Ğ³ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°

        for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            if (!content) continue;

            chunkCount++;
            if (!firstChunkTime) {
                firstChunkTime = Date.now();
                const timeToFirstChunk = firstChunkTime - gptStartTime;
                console.log(`[${new Date().toISOString()}] âœ… First GPT chunk received in ${timeToFirstChunk}ms`);
            }

            accumulatedText += content;
            fullText += content;

            // ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ° TTS
            // Ğ‘Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°: Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ»ÑĞ±Ğ¾Ğ¼ Ğ·Ğ½Ğ°ĞºĞµ Ğ¿Ñ€ĞµĞ¿Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°
            const hasPunctuation = /[.!?,;:]\s*$/.test(accumulatedText);
            const isLongEnough = accumulatedText.length >= MIN_CHARS_FOR_TTS;
            const isTooLong = accumulatedText.length >= MAX_WAIT_CHARS;
            
            // Ğ”Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°: Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞµÑ‰Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ (Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°ĞºĞµ Ğ¿Ñ€ĞµĞ¿Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ)
            const firstChunkThreshold = isFirstTtsChunk ? 6 : MIN_CHARS_FOR_TTS;
            const shouldSendFirstChunk = isFirstTtsChunk && (accumulatedText.length >= firstChunkThreshold || hasPunctuation);

            // ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ: Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°ĞºĞµ Ğ¿Ñ€ĞµĞ¿Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ + Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°, Ğ¸Ğ»Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°
            if (shouldSendFirstChunk || (hasPunctuation && isLongEnough) || isTooLong) {
                // ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° TTS
                const textToTTS = accumulatedText.trim();
                if (textToTTS) {
                    accumulatedText = ''; // Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
                    const isFirst = isFirstTtsChunk;
                    if (isFirstTtsChunk) {
                        isFirstTtsChunk = false; // Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ñ„Ğ»Ğ°Ğ³ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ°
                    }
                    
                    // Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
                    ttsQueue = ttsQueue.then(async () => {
                        try {
                            const ttsStartTime = Date.now();
                            ttsChunkCount++;
                            console.log(`[${new Date().toISOString()}] ğŸ”Š [TTS #${ttsChunkCount}] Generating for: "${textToTTS.substring(0, 50)}..."`);
                            
                            const ttsResponse = await openai.audio.speech.create({
                                model: 'tts-1',
                                voice: 'alloy',
                                input: textToTTS,
                                response_format: 'mp3',
                                speed: 1.0
                            });

                            const audioChunk = Buffer.from(await ttsResponse.arrayBuffer());
                            const ttsDuration = Date.now() - ttsStartTime;
                            
                            if (!firstTtsSentTime) {
                                firstTtsSentTime = Date.now();
                                const totalTime = firstTtsSentTime - gptStartTime;
                                console.log(`[${new Date().toISOString()}] ğŸµ FIRST AUDIO CHUNK SENT! Total time: ${totalTime}ms (${isFirst ? 'FIRST CHUNK OPTIMIZATION' : 'normal'})`);
                            }
                            
                            console.log(`[${new Date().toISOString()}] âœ… [TTS #${ttsChunkCount}] Generated ${audioChunk.length} bytes in ${ttsDuration}ms`);
                            
                            // ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡Ğ°Ğ½Ğº ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ
                            if (!res.headersSent) {
                                res.setHeader('Transfer-Encoding', 'chunked');
                            }
                            res.write(audioChunk);
                            console.log(`[${new Date().toISOString()}] ğŸ“¤ [TTS #${ttsChunkCount}] Chunk sent to client`);
                        } catch (error) {
                            console.error(`[${new Date().toISOString()}] âŒ Error generating TTS chunk:`, error);
                        }
                    });
                }
            }
        }

        // ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ñ‚ĞµĞºÑÑ‚Ğ°
        if (accumulatedText.trim()) {
            const textToTTS = accumulatedText.trim();
            
            ttsQueue = ttsQueue.then(async () => {
                console.log(`[${new Date().toISOString()}] Generating final TTS chunk...`);
                const ttsResponse = await openai.audio.speech.create({
                    model: 'tts-1',
                    voice: 'alloy',
                    input: textToTTS,
                    response_format: 'mp3',
                    speed: 1.0
                });

                const audioChunk = Buffer.from(await ttsResponse.arrayBuffer());
                if (!res.headersSent) {
                    res.setHeader('Transfer-Encoding', 'chunked');
                }
                res.write(audioChunk);
            });
        }
        
        // Ğ–Ğ´ĞµĞ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… TTS Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
        await ttsQueue;

        const totalTime = Date.now() - gptStartTime;
        console.log(`[${new Date().toISOString()}] ğŸ“Š Streaming Summary:`);
        console.log(`  - GPT chunks received: ${chunkCount}`);
        console.log(`  - TTS chunks sent: ${ttsChunkCount}`);
        console.log(`  - Time to first chunk: ${firstChunkTime ? firstChunkTime - gptStartTime : 0}ms`);
        console.log(`  - Time to first audio: ${firstTtsSentTime ? firstTtsSentTime - gptStartTime : 0}ms`);
        console.log(`  - Total processing time: ${totalTime}ms`);
        console.log(`[${new Date().toISOString()}] Full assistant response: "${fullText.trim()}"`);

        if (!fullText.trim()) {
            throw new Error('ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°');
        }

        // Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        addToHistory(sessionId, 'assistant', fullText.trim());

        // Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
        res.end();

    } catch (error) {
        console.error(`[${new Date().toISOString()}] Error processing voice:`, error);
        if (!res.headersSent) {
            res.setHeader('Access-Control-Allow-Origin', '*');
            res.status(500).json({ 
                error: 'Failed to process voice request',
                message: error.message
            });
        } else {
            res.end();
        }
        throw error;
    }
}

// ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° OPTIONS Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
app.options('/api/voice', (req, res) => {
    res.header('Access-Control-Allow-Origin', '*');
    res.header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
    res.header('Access-Control-Allow-Headers', 'Content-Type, Authorization, X-Session-Id, Accept');
    res.header('Access-Control-Max-Age', '86400');
    res.sendStatus(200);
});

// Middleware Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
app.use((req, res, next) => {
    console.log(`[${new Date().toISOString()}] ${req.method} ${req.path}`);
    next();
});

// Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ endpoint
app.get('/api/voice', (req, res) => {
    res.setHeader('Access-Control-Allow-Origin', '*');
    res.json({
        message: 'OpenAI Voice Assistant API',
        method: 'POST required',
        description: 'This endpoint accepts POST requests with audio file',
        usage: {
            method: 'POST',
            contentType: 'multipart/form-data',
            fields: {
                file: 'Audio file (WebM, MP3, WAV, M4A, etc.)',
                sessionId: 'Session ID (optional, for conversation history)'
            }
        },
        endpoints: {
            voice: '/api/voice (POST)',
            health: '/health (GET)'
        }
    });
});

// ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ endpoint Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
app.post('/api/voice', upload.single('file'), async (req, res) => {
    const requestStartTime = Date.now();
    
    if (!req.file) {
        console.error(`[${new Date().toISOString()}] ERROR: No audio file provided`);
        res.setHeader('Access-Control-Allow-Origin', '*');
        return res.status(400).json({ error: 'No audio file provided' });
    }

    if (!process.env.OPENAI_API_KEY) {
        res.setHeader('Access-Control-Allow-Origin', '*');
        return res.status(500).json({ error: 'OpenAI API key not configured' });
    }

    const sessionId = req.body.sessionId || req.headers['x-session-id'] || 'default';
    
    console.log(`[${new Date().toISOString()}] Processing voice request:`);
    console.log(`  - Session ID: ${sessionId}`);
    console.log(`  - File size: ${req.file.size} bytes`);
    console.log(`  - File type: ${req.file.mimetype || 'unknown'}`);

    try {
        // Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ streaming Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
        res.setHeader('Access-Control-Allow-Origin', '*');
        res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
        res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization, X-Session-Id, Accept');
        res.setHeader('Content-Type', 'audio/mpeg');
        res.setHeader('X-Session-Id', sessionId);
        res.setHeader('Cache-Control', 'no-cache');
        res.setHeader('Connection', 'keep-alive');
        res.setHeader('Transfer-Encoding', 'chunked');

        // ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ ÑĞ¾ streaming
        await processVoiceRequestStreaming(req.file.buffer, sessionId, res);

            const duration = Date.now() - requestStartTime;
        console.log(`[${new Date().toISOString()}] Request completed for session: ${sessionId} (took ${duration}ms)`);

    } catch (error) {
        const duration = Date.now() - requestStartTime;
        console.error(`[${new Date().toISOString()}] ERROR after ${duration}ms:`, {
            message: error.message,
            stack: error.stack
        });
        
        if (!res.headersSent) {
            res.setHeader('Access-Control-Allow-Origin', '*');
            res.status(500).json({ 
                error: 'Failed to process voice request',
                message: error.message
            });
        }
    }
});

// Health check endpoint
app.get('/health', (req, res) => {
    res.json({ 
        status: 'ok', 
        timestamp: new Date().toISOString(),
        openai_configured: !!process.env.OPENAI_API_KEY
    });
});

// Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²ĞµÑ€Ğ°
app.listen(PORT, () => {
    console.log(`ğŸš€ OpenAI Voice Assistant Server running on port ${PORT}`);
    console.log(`ğŸ¤ Voice endpoint: http://localhost:${PORT}/api/voice`);
    console.log(`ğŸ“Š Health check: http://localhost:${PORT}/health`);
    if (!process.env.OPENAI_API_KEY) {
        console.log(`âš ï¸  WARNING: Set OPENAI_API_KEY environment variable!`);
    }
});
